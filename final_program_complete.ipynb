{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ab6a8-2a5f-47c9-9d9a-a3ad5c9a1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings # Supress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from joblib import Parallel, delayed\n",
    "import glob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ecdb3-d13e-48c4-ae5a-fd7301a1ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- # \n",
    "# DATA LOADING AFTER FACTOR SELECTION \n",
    "# -------------------------------- #\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "pd.set_option(\"mode.chained_assignment\", None)\n",
    "\n",
    "work_dir = \"/teamspace/studios/this_studio/\"\n",
    "\n",
    "# Load factor list\n",
    "factor_csv_path = os.path.join(work_dir, \"Quant_Data_Labels/selected_factors.csv\") #changed from factor_char_list.csv to selected_factors.csv\n",
    "factor_list_raw = pd.read_csv(factor_csv_path)\n",
    "factor_list = factor_list_raw[\"factor\"].tolist() if \"factor\" in factor_list_raw.columns else factor_list_raw.iloc[:, 0].tolist() #column for csv is variable. Changed variable to factor as that is what is used in selected_factors\n",
    "\n",
    "# Determine which factors exist in Parquet\n",
    "file_path = os.path.join(work_dir, \"Actual_Data/ret_sample.parquet\")\n",
    "parquet_cols = pd.read_parquet(file_path, columns=None).columns.tolist()\n",
    "factors_to_load = [f for f in factor_list if f in parquet_cols]\n",
    "\n",
    "# Load only needed columns\n",
    "cols_to_load = [\"date\", \"gvkey\",\"ret_eom\",\"year\",\"month\",\"stock_ret\",\"id\"] + factors_to_load #added in more identifiers\n",
    "data = pd.read_parquet(file_path, columns=cols_to_load)\n",
    "data = data[data[\"stock_ret\"].notna()]\n",
    "stock_vars = factors_to_load\n",
    "ret_var = \"stock_ret\"\n",
    "\n",
    "# Monthly scaling & ranking\n",
    "monthly = data.groupby(\"date\")\n",
    "\n",
    "def process_month(group_tuple):\n",
    "    _, group = group_tuple\n",
    "\n",
    "    # Fill missing with monthly median\n",
    "    medians = group[stock_vars].median(skipna=True)\n",
    "    group[stock_vars] = group[stock_vars].fillna(medians)\n",
    "\n",
    "    # Rank variables\n",
    "    group[stock_vars] = group[stock_vars].rank(method=\"dense\") - 1\n",
    "\n",
    "    # Scale to [-1, 1]\n",
    "    for stock in stock_vars:\n",
    "        max_val = group[stock].max()\n",
    "        group[stock] = (group[stock] / max_val) * 2 - 1 if max_val > 0 else 0\n",
    "\n",
    "    return group\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    result = list(tqdm(executor.map(process_month, monthly), total=len(monthly)))\n",
    "\n",
    "data = pd.concat(result, ignore_index=True) #ignores original index and creates new index\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"], format=\"%Y%m%d\")\n",
    "\n",
    "print(\"Data loaded and preprocessed:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680beccf-71a3-42ea-bcd5-2ad7e7e3c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- # \n",
    "# DATA LOADING (BEFORE FACTOR SELECTION) \n",
    "# -------------------------------- #\n",
    "# for timing purpose\n",
    "print(datetime.datetime.now())\n",
    "# Supress numpy runtime warnings\n",
    "# turn off pandas Setting with Copy Warning\n",
    "pd.set_option(\"mode.chained_assignment\", None)\n",
    "\n",
    "# set working directory\n",
    "work_dir = \"/teamspace/studios/this_studio/\"\n",
    "\n",
    "# read sample data\n",
    "file_path = os.path.join(\n",
    "    work_dir, \"Actual_Data/ret_sample.parquet\"\n",
    ")  # replace with the correct file name\n",
    "# print(os.path.exists(file_path))\n",
    "\n",
    "raw = pd.read_parquet(\n",
    "    file_path \n",
    ") # the date is the first day of the return month (t+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read list of predictors for stocks\n",
    "file_path = os.path.join(\n",
    "    work_dir, \"Quant_Data_Labels/factor_char_list.csv\"\n",
    ")  # replace with the correct file name\n",
    "stock_vars = list(pd.read_csv(file_path)[\"variable\"].values)\n",
    "# define the left hand side variable\n",
    "ret_var = \"stock_ret\" #not normailzing the target on the predictive factors!!!\n",
    "new_set = raw[\n",
    "    raw[ret_var].notna()\n",
    "].copy()  # create a copy of the data and make sure the left hand side is not missing\n",
    "\n",
    "# transform each variable in each month to the same scale\n",
    "monthly = new_set.groupby(\"date\")\n",
    "data = pd.DataFrame()\n",
    "\n",
    "def parallel(groups):\n",
    "    _,group = groups\n",
    "    group = group.copy()\n",
    "    \n",
    "    var_median = group[stock_vars].median(skipna=True)\n",
    "    group[stock_vars] = group[stock_vars].fillna(var_median)\n",
    "\n",
    "    group[stock_vars] = group[stock_vars].rank(method=\"dense\")-1\n",
    "    \n",
    "    for stock in stock_vars:\n",
    "        stock_max = group[stock].max()\n",
    "        if stock_max > 0:\n",
    "            group[stock] = (group[stock] / stock_max) * 2 - 1\n",
    "        else:\n",
    "            group[stock] = 0  # in case of all missing values\n",
    "            #print(\"Warning:\", date[stock], stock, \"set to zero.\") # Print command\n",
    "    return group\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "\n",
    "    result = list(tqdm(executor.map(parallel, monthly), total=len(monthly)))\n",
    "    \n",
    "data = pd.concat(result, ignore_index = True) #ignores original index and creates new index\n",
    "\n",
    "# Convert format of the date to a Timestamp\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"], format=\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442986c-8255-4422-9cdd-4e26da30cbd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Factor selection (once for full dataset)\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# Load factor list from CSV\n",
    "# -----------------------------\n",
    "factor_csv_path = os.path.join(work_dir, \"Quant_Data_Labels/factor_char_list.csv\")\n",
    "factor_list_raw = pd.read_csv(factor_csv_path)\n",
    "# If the CSV has a column header, use it; otherwise fallback to first column\n",
    "if \"factor\" in factor_list_raw.columns:\n",
    "    factor_list = factor_list_raw[\"factor\"].tolist()\n",
    "else:\n",
    "    factor_list = factor_list_raw.iloc[:, 0].tolist()\n",
    "\n",
    "def select_factors(data, factor_list, ret_var, missing_thresh=0.5,\n",
    "                   corr_thresh=0.95, lgb_n_estimators=100,\n",
    "                   corr_sample_size=50000, lgb_sample_size=50000,\n",
    "                   random_state=42):\n",
    "    # -----------------------------\n",
    "    # 1. Clean factor list\n",
    "    # -----------------------------\n",
    "    factors_clean = [f for f in factor_list if f in data.columns]\n",
    "    if not factors_clean:\n",
    "        print(\"No factors found in data.\")\n",
    "        return []\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Remove highly correlated factors\n",
    "    # -----------------------------\n",
    "    n_corr_sample = min(corr_sample_size, len(data))\n",
    "    corr_idx = np.random.RandomState(random_state).choice(data.index, size=n_corr_sample, replace=False)\n",
    "    X_corr_sample = data.loc[corr_idx, factors_clean].fillna(0)\n",
    "    corr_matrix = X_corr_sample.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [c for c in upper.columns if any(upper[c] > corr_thresh)]\n",
    "    factors_corr = [f for f in factors_clean if f not in to_drop]\n",
    "    print(f\"{len(factors_corr)} factors remain after removing highly correlated factors (>{corr_thresh})\")\n",
    "    if not factors_corr:\n",
    "        return []\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Sample for LightGBM\n",
    "    # -----------------------------\n",
    "    n_lgb_sample = min(lgb_sample_size, len(data))\n",
    "    lgb_idx = np.random.RandomState(random_state + 1).choice(data.index, size=n_lgb_sample, replace=False)\n",
    "    X_lgb_sample = data.loc[lgb_idx, factors_corr].fillna(0)\n",
    "    Y_raw = pd.to_numeric(data.loc[lgb_idx, ret_var], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    if Y_raw.nunique() <= 1:\n",
    "        print(\"Target is constant. Skipping LightGBM.\")\n",
    "        return factors_corr  # fallback\n",
    "\n",
    "    Y_bin = (Y_raw > 0).astype(int)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. LightGBM feature importance\n",
    "    # -----------------------------\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=lgb_n_estimators,\n",
    "        n_jobs=-1,\n",
    "        verbose=10\n",
    "    )\n",
    "    lgb_model.fit(X_lgb_sample, Y_bin)\n",
    "    factors_final = [f for f, imp in zip(factors_corr, lgb_model.feature_importances_) if imp > 0]\n",
    "    print(f\"{len(factors_final)} factors selected by LightGBM importance (>0)\")\n",
    "\n",
    "    return factors_final\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Precompute factors and save\n",
    "# -----------------------------\n",
    "full_train = data[data[\"date\"] < pd.to_datetime(\"20250101\")]\n",
    "selected_factors = select_factors(full_train, factor_list, ret_var=\"stock_ret\", corr_thresh=0.75) \n",
    "print(f\"Precomputed factors: {len(selected_factors)}\")\n",
    "\n",
    "factors_file = os.path.join(work_dir, \"Quant_Data_Labels/selected_factors.csv\")\n",
    "# os.makedirs(os.path.dirname(factors_file), exist_ok=True)\n",
    "pd.DataFrame(selected_factors, columns=[\"factor\"]).to_csv(factors_file, index=False)\n",
    "print(f\"Factors saved to {factors_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be2dea-cb4e-437f-9cf6-0af987590858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Transformer classifier\n",
    "# -----------------------------\n",
    "class SimpleTransformerClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, n_heads=4, hidden_dim=64, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.cls_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.input_proj(x)\n",
    "        h = self.encoder(h)\n",
    "        h = h.mean(dim=1)  # mean pooling over sequence\n",
    "        logit = self.cls_head(h)\n",
    "        return torch.sigmoid(logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e256207",
   "metadata": {},
   "source": [
    "IMPLEMENTING finBERT LLM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SETTINGS\n",
    "# -----------------------------\n",
    "work_dir = \"/teamspace/studios/this_studio/\"\n",
    "PKL_DIR = os.path.join(work_dir, \"Text_Data_picklefiles\")\n",
    "LINK_CSV = os.path.join(work_dir, \"Quant_Data_Labels/cik_gvkey_linktable_USA_only.csv\")\n",
    "CACHE_DIR = os.path.join(PKL_DIR, \"finbert_forward_by_gvkey_fast\")\n",
    "complete_parquet = os.path.join(work_dir, \"Actual_Data/risk_score_256.parquet\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL = \"ProsusAI/finbert\"\n",
    "MAX_LEN = 256        # 256 tokens per filing --> may need to increase this as probably truncating long text fillings\n",
    "BATCH_TXT = 128       \n",
    "PRINT_EVERY = 10000    # progress prints\n",
    "complete_parquet = os.path.join(work_dir, f\"Actual_Data/risk_score_{MAX_LEN}.parquet\")\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD FAST TOKENIZER & MODEL\n",
    "# -----------------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(MODEL).to(DEVICE).eval()\n",
    "try:\n",
    "    mdl.half()  # converting to 16bit for memory purposes\n",
    "except:\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def fast_score_texts(texts):\n",
    "    \n",
    "    enc = tok(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "    enc = {k: v.to(dtype=torch.int64) if k==\"input_ids\" else v for k,v in enc.items()}\n",
    "    logits = mdl(**enc).logits.half()  # FP16\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    return probs[:,0] - probs[:,2]  # risk = p_neg - p_pos\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD LINK TABLE\n",
    "# -----------------------------\n",
    "def load_link_table(path):\n",
    "    lt = pd.read_csv(path)\n",
    "    lt.columns = [c.lower() for c in lt.columns]\n",
    "    lt[\"cik\"] = lt[\"cik\"].astype(str).str.strip()\n",
    "    lt[\"gvkey\"] = lt[\"gvkey\"].astype(str).str.replace(r\"\\.0$\",\"\",regex=True).str.strip()\n",
    "    lt[\"linkdt\"] = pd.NaT #assuming each gvkey and cik are unique\n",
    "    lt[\"linkenddt\"] = pd.NaT\n",
    "    return lt[[\"cik\",\"gvkey\",\"linkdt\",\"linkenddt\"]].copy()\n",
    "\n",
    "link = load_link_table(LINK_CSV)\n",
    "\n",
    "# -----------------------------\n",
    "# PROCESS FILINGS YEAR-BY-YEAR\n",
    "# -----------------------------\n",
    "def build_finbert_by_year(pkl_dir=PKL_DIR, link_table=link, cache_dir=CACHE_DIR):\n",
    "    all_files = sorted(glob.glob(os.path.join(pkl_dir, \"text_us_*.pkl\")))\n",
    "    all_years_data = []\n",
    "\n",
    "    for fp in all_files:\n",
    "        year = os.path.basename(fp).split(\"_\")[2].split(\".\")[0]\n",
    "        cache_file = os.path.join(cache_dir, f\"finbert_{year}.parquet\")\n",
    "        if os.path.exists(cache_file): #saves cached files and loads them seperately which is good due to maximum session run times.\n",
    "            print(f\"Year {year} cached, loading...\")\n",
    "            all_years_data.append(pd.read_parquet(cache_file))\n",
    "            continue\n",
    "\n",
    "        df0 = pd.read_pickle(fp)\n",
    "        if \"date\" not in df0 or \"cik\" not in df0:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"cik\": df0[\"cik\"].astype(str).str.strip(),\n",
    "            \"filing_date\": pd.to_datetime(df0[\"date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\"),\n",
    "            \"text\": (df0.get(\"mgmt\",\"\").fillna(\"\") + \" \" + df0.get(\"rf\",\"\").fillna(\"\")).str.replace(r\"\\s+\",\" \",regex=True).str.strip()\n",
    "        }).dropna(subset=[\"cik\",\"filing_date\"])\n",
    "        df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "        # Batch inference\n",
    "        risks = []\n",
    "        n_batches = int(np.ceil(len(df) / BATCH_TXT))\n",
    "        for i in range(n_batches):\n",
    "            batch_texts = df[\"text\"].iloc[i*BATCH_TXT:(i+1)*BATCH_TXT].tolist()\n",
    "            risks.extend(fast_score_texts(batch_texts))\n",
    "            if i % (PRINT_EVERY // BATCH_TXT) == 0:\n",
    "                print(f\"Year {year}: batch {i+1}/{n_batches} ({len(risks)}/{len(df)}) filings scored\")\n",
    "\n",
    "        df[\"finbert_risk\"] = np.array(risks, dtype=float)\n",
    "\n",
    "        # Merge with gvkey\n",
    "        m = df.merge(link_table, on=\"cik\", how=\"left\")\n",
    "        m = m.dropna(subset=[\"gvkey\"])\n",
    "        m = m.sort_values([\"cik\",\"filing_date\"]).drop_duplicates([\"cik\",\"filing_date\"], keep=\"last\")\n",
    "\n",
    "        # Monthly aggregation. Not forward aligning using change from previous months data\n",
    "        m[\"year\"] = m[\"filing_date\"].dt.year\n",
    "        m[\"month\"] = m[\"filing_date\"].dt.month\n",
    "        risk_m = m.groupby([\"gvkey\",\"year\",\"month\"], as_index=False)[\"finbert_risk\"].mean()\n",
    "        risk_m[\"finbert_risk_mom\"] = risk_m.groupby(\"gvkey\")[\"finbert_risk\"].diff()\n",
    "     \n",
    "        # Cache\n",
    "        risk_m.to_parquet(cache_file, index=False) \n",
    "        all_years_data.append(risk_fwd)\n",
    "        print(f\"Year {year} done. Saved to {cache_file}\")\n",
    "\n",
    "\n",
    "    return pd.concat(all_years_data, ignore_index=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# RUN FINBERT SCORING AND CREATE RISK FILE\n",
    "# ----------------------------------------\n",
    "risk = build_finbert_by_year()\n",
    "\n",
    "# -----------------------------\n",
    "# MERGE WITH MAIN DATA\n",
    "# -----------------------------\n",
    "data = data.merge(\n",
    "    risk,\n",
    "    on=[\"gvkey\",\"year\",\"month\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#section adds an indicator to differentiate between having a zero sentiment to being empty. The original code just fillna(0.0) for everything that is empty/NaN\n",
    "data[\"has_filing\"] = (~data[[\"finbert_risk_fwd\", \"finbert_risk_mom_fwd\"]].isna().any(axis=1)).astype(int)\n",
    "\n",
    "data[\"finbert_risk_fwd\"] = data[\"finbert_risk_fwd\"].where(data[\"has_filing\"]==1, 0.0)\n",
    "data[\"finbert_risk_mom_fwd\"] = data[\"finbert_risk_mom_fwd\"].where(data[\"has_filing\"]==1, 0.0)\n",
    "\n",
    "\n",
    "# Add to factor list\n",
    "for c in [\"finbert_risk_fwd\", \"finbert_risk_mom_fwd\", \"has_filing\"]:\n",
    "    if c not in factor_list:\n",
    "        factor_list.append(c)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c020307-71f6-468b-8111-2894a2261b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Expanding-window prediction\n",
    "# -----------------------------\n",
    "starting = pd.to_datetime(\"20050101\", format=\"%Y%m%d\")\n",
    "counter = 0\n",
    "pred_out = pd.DataFrame()\n",
    "ret_var = \"stock_ret\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256 #increase BATCH_SIZE as appropriate for system used. Multiple GPUs can handle a higher BATCH_SIZE which allows for faster runtime\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "window_train_years = 8\n",
    "window_val_years = 2\n",
    "\n",
    "while (starting + pd.DateOffset(years=window_train_years + window_val_years + 1 + counter)) <= pd.to_datetime(\"20260101\"):\n",
    "    cutoff_train_start = starting\n",
    "    cutoff_train_end = starting + pd.DateOffset(years=window_train_years + counter)\n",
    "    cutoff_val_end = cutoff_train_end + pd.DateOffset(years=window_val_years)\n",
    "    cutoff_test_end = cutoff_val_end + pd.DateOffset(years=1)\n",
    "\n",
    "    train = data[(data[\"date\"] >= cutoff_train_start) & (data[\"date\"] < cutoff_train_end)]\n",
    "    validate = data[(data[\"date\"] >= cutoff_train_end) & (data[\"date\"] < cutoff_val_end)]\n",
    "    test = data[(data[\"date\"] >= cutoff_val_end) & (data[\"date\"] < cutoff_test_end)]\n",
    "# -----------------------------\n",
    "    # Standardize features\n",
    "    # -----------------------------\n",
    "    # Assuming 'data' already contains only useful factors\n",
    "    factors_in_data = [f for f in factor_list if f in data.columns]\n",
    "    scaler = StandardScaler().fit(train[factors_in_data])\n",
    "    X_train = scaler.transform(train[factors_in_data])\n",
    "    X_val = scaler.transform(validate[factors_in_data])\n",
    "    X_test = scaler.transform(test[factors_in_data])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prepare classification target\n",
    "    # -----------------------------\n",
    "    Y_train_bin = (train[ret_var].values > 0).astype(int) #change all of these from ret_1_0 to stock_ret\n",
    "    Y_val_bin = (validate[ret_var].values > 0).astype(int)\n",
    "    Y_test_bin = (test[ret_var].values > 0).astype(int)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Transformer classifier\n",
    "    # -----------------------------\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val_t = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "    Y_train_t = torch.tensor(Y_train_bin, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_t, Y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleTransformerClassifier(feature_dim=len(factors_in_data)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prob = model(xb)\n",
    "            loss = criterion(prob, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Inference + isotonic calibration\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_probs_raw = model(X_val_t.to(device)).cpu().numpy().flatten()\n",
    "        test_probs_raw = model(X_test_t.to(device)).cpu().numpy().flatten()\n",
    "\n",
    "    iso = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso.fit(val_probs_raw, Y_val_bin)\n",
    "    test_probs_cal = iso.predict(test_probs_raw)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Conditional expected return prediction\n",
    "    # -----------------------------\n",
    "    X_train_pos = X_train[train[ret_var] > 0] #previously ret_1_0 for all of these\n",
    "    Y_train_pos = train.loc[train[ret_var] > 0, ret_var].values\n",
    "    X_train_neg = X_train[train[ret_var] <= 0]\n",
    "    Y_train_neg = train.loc[train[ret_var] <= 0, ret_var].values\n",
    "\n",
    "    lgb_pos = lgb.LGBMRegressor()\n",
    "    lgb_pos.fit(X_train_pos, Y_train_pos)\n",
    "    lgb_neg = lgb.LGBMRegressor()\n",
    "    lgb_neg.fit(X_train_neg, Y_train_neg)\n",
    "\n",
    "    mu_test = test_probs_cal * lgb_pos.predict(X_test) + \\\n",
    "              (1 - test_probs_cal) * lgb_neg.predict(X_test)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Store outputs\n",
    "    # -----------------------------\n",
    "    reg_pred = test[[\"year\", \"month\", \"stock_ret\", \"id\", \"stock_ret\"]].copy() #switched ret_1_0 to stock_ret. ID not in index.\n",
    "    reg_pred[\"transformer_prob_raw\"] = test_probs_raw\n",
    "    reg_pred[\"transformer_prob_cal\"] = test_probs_cal\n",
    "    reg_pred[\"exp_ret\"] = mu_test\n",
    "    pred_out = pred_out._append(reg_pred, ignore_index=True)\n",
    "    counter += 1\n",
    "    print(f\"Window {counter} done, {len(factors_in_data)} factors used.\") \n",
    "\n",
    "# -----------------------------\n",
    "# Output results\n",
    "# -----------------------------\n",
    "pred_out.to_csv(\"output_transformer_lgb_precomputed_factors.csv\", index=False)\n",
    "print(\"done!\")\n",
    "\n",
    "# -----------------------------\n",
    "# OOS R2 -> Model predictivity\n",
    "# -----------------------------\n",
    "\n",
    "yreal = pred_out[ret_var].to_numpy().flatten() #previously ret_1_0\n",
    "ypred = pred_out[\"exp_ret\"].to_numpy().flatten()\n",
    "\n",
    "oos_r2 = 1 - np.sum((yreal - ypred)**2) / np.sum(yreal**2)\n",
    "\n",
    "print(\"OOS R^2:\", oos_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee22cd1",
   "metadata": {},
   "source": [
    "PORTFOLIO OPTIMIZATION CODE FOLLOWING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d901820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Mean Variance Portfolio Optimization - Markowitz portfolio optimization using cvxpy library for convex optimization problems (not including monthly turnover)\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#this method is instead of splitting the stocks into 10 portfolios and ranking them. Shorting the lowest and long the highest. This is providing weights to each stock and then choosing the optimal porfolio\n",
    "\n",
    "import cvxpy as cp\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a085c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Prepare predicted returns\n",
    "# ---------------------------\n",
    "\n",
    "pred_returns = pred_out.pivot(index=\"date\", columns=\"id\", values=\"exp_ret\").fillna(0)\n",
    "\n",
    "pred_returns[\"year_month\"] = pred_returns[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "pred_ret_monthly = pred_returns.groupby(\"year_month\")\n",
    "\n",
    "mu = np.array(pred_returns.mean())\n",
    "\n",
    "covMatrix = pred_returns.cov().values\n",
    "\n",
    "noa = pred_returns.shape[1] #determining the number of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04510965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Set up optimization problem\n",
    "# ---------------------------\n",
    "\n",
    "w = cp.Variable(noa) #creating a variable for the weights \n",
    "\n",
    "gamma = 3 #Higher gamma meens more risk adverse. Lower gamma meens taking on more risk\n",
    "\n",
    "ret = mu @ w\n",
    "risk = cp.quad_form(w, covMatrix)\n",
    "\n",
    "goal = cp.Maximize((ret - risk*gamma))\n",
    "\n",
    "constraints = [ #can add constraint w>=0 would give a long only portfolio. Maybe look into different possible constraints that could make the optimization better\n",
    "    cp.sum(w) == 1 #fully invested\n",
    "]\n",
    "\n",
    "problem = cp.Problem(goal, constraints) #not sure why I am getting error with constraints type but I think it is just because files are not loaded\n",
    "problem.solve(solver=cp.SCS)\n",
    "\n",
    "#weights_opt = pd.DataFrame( Adding weights to the dataframe pred_returns. May be useful.\n",
    "   # w.value, index=pred_returns.columns, columns=[\"Weight\"]\n",
    "#)\n",
    "#print(weights_opt.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Compute portfolio returns performace metrics\n",
    "# -----------------------------------------------\n",
    "\n",
    "portfolio_returns = np.dot(pred_returns.values, w.value)\n",
    "\n",
    "cum_ret = (1+portfolio_returns).cumprod()-1\n",
    "\n",
    "mkt_csv_path = os.path.join(work_dir, \"Quant_Data_Labels/mkt_factor.csv\")\n",
    "\n",
    "portfolio_df = pd.DataFrame({\n",
    "    \"date\": pred_returns.index,\n",
    "    \"portfolio\": portfolio_returns\n",
    "})\n",
    "\n",
    "portfolio_df['year'] = portfolio_df['date'].dt.year #extracting the year\n",
    "portfolio_df['month'] = portfolio_df['date'].dt.month #extracting the date\n",
    "\n",
    "\n",
    "\n",
    "def SharpeRatio(returns):\n",
    "    return returns.mean() / returns.std() * np.sqrt(12)\n",
    "\n",
    "def maxDrawDown(cum_returns):\n",
    "    running_max = np.maximum.accumulate(cum_returns + 1)\n",
    "    dd = (cum_returns + 1) / running_max - 1\n",
    "    return dd.max()\n",
    "\n",
    "def annualizedReturn(returns):\n",
    "    return returns.mean()*12 #multiply the average monthly returns by 12 to get the average yearly return\n",
    "\n",
    "def anualizedStd(returns):\n",
    "    return returns.std() * 12\n",
    "\n",
    "def maxmonthLoss(returns):\n",
    "    return returns.min() #np.min(returns)\n",
    "\n",
    "def annualizedAlphaInfoTstat(returns_df, file_path):\n",
    "    mkt = pd.read_csv(file_path)\n",
    "\n",
    "    mkt['year'] = mkt['year'].astype(int)\n",
    "    mkt['month'] = mkt['month'].astype(int)\n",
    "\n",
    "    merged_df = returns_df.merge(mkt, how=\"inner\", on=[\"year\", \"month\"])\n",
    "\n",
    "    merged_df['port_excess'] = merged_df['portfolio'] - merged_df['rf']\n",
    "\n",
    "    nw_ols = sm.ols(formula=\"port_excess ~ mkt_rf\", data=merged_df).fit(\n",
    "    cov_type=\"HAC\", cov_kwds={\"maxlags\": 3}, use_t=True\n",
    "    )\n",
    "    \n",
    "    alpha = nw_ols.parms[\"Intercept\"] * 12 #annualized alpha\n",
    "    \n",
    "    t_statistic = nw_ols.tvales[\"Intercept\"] #t statistic \n",
    "    \n",
    "    Info_Ratio = alpha/(np.sqrt(nw_ols.mse_resid) * np.sqrt(12)) #annualized info_ratio\n",
    "\n",
    "    return alpha,t_statistic,Info_Ratio\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "Sharpe_Annualized = SharpeRatio(portfolio_returns)\n",
    "max_drawdown = maxDrawDown(cum_ret)\n",
    "annual_return = annualizedReturn(portfolio_returns)\n",
    "annaul_std = anualizedStd(portfolio_returns)\n",
    "max_loss = maxmonthLoss(portfolio_returns)\n",
    "annualized_data = annualizedAlphaInfoTstat(portfolio_df, mkt_csv_path)\n",
    "\n",
    "print(f\"The annualized Sharpe ratio is {Sharpe_Annualized:.4f} and the maximum drawdown is {max_drawdown:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f327b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Portfolio optimization including monthly portfolio turnover\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cvxpy as cp\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1. Prepare data\n",
    "# -----------------------------------------------\n",
    "\n",
    "pred_returns = pred_out.pivot(index=\"date\", columns=\"id\", values=\"exp_ret\").fillna(0)\n",
    "pred_returns.index = pd.to_datetime(pred_returns.index)\n",
    "pred_returns[\"year_month\"] = pred_returns.index.to_period(\"M\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2. Setting up problem\n",
    "# -----------------------------------------------\n",
    "\n",
    "gamma = 3\n",
    "weights_prev = None\n",
    "portfolio_history = []\n",
    "turnover_list = []\n",
    "dates_list = []\n",
    "\n",
    "for ym, month_data in pred_returns.groupby(\"year_month\"):\n",
    "    month_data = month_data.drop(columns=\"year_month\")\n",
    "    mu = month_data.mean().values\n",
    "    covMatrix = month_data.cov().values\n",
    "    noa = month_data.shape[1]\n",
    "\n",
    "    w = cp.Variable(noa)\n",
    "    ret = mu @ w\n",
    "    risk = cp.quad_form(w, covMatrix)\n",
    "    objective = cp.Maximize(ret - gamma * risk)\n",
    "    constraints = [cp.sum(w) == 1] #fully invested. Not long only.\n",
    "\n",
    "    if weights_prev is not None:\n",
    "        turnover = cp.norm1(w - weights_prev)\n",
    "        constraints.append(turnover <= 0.2)\n",
    "\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.SCS, verbose=False)\n",
    "\n",
    "    if w.value is None:\n",
    "        print(f\"Optimization failed for {ym}\")\n",
    "        continue\n",
    "\n",
    "    weights = np.array(w.value)\n",
    "    portfolio_history.append(weights)\n",
    "    dates_list.append(ym)\n",
    "\n",
    "    if weights_prev is not None:\n",
    "        monthly_turnover = np.sum(np.abs(weights - weights_prev))\n",
    "        turnover_list.append(monthly_turnover)\n",
    "    else:\n",
    "        turnover_list.append(np.nan)\n",
    "\n",
    "    weights_prev = weights\n",
    "\n",
    "weights_df = pd.DataFrame(portfolio_history, index=dates_list, columns=month_data.columns)\n",
    "turnover_series = pd.Series(turnover_list, index=dates_list, name=\"turnover\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3. Compute monthly returns\n",
    "# -----------------------------------------------\n",
    "\n",
    "portfolio_returns = (pred_returns.drop(columns=\"year_month\") * weights_df.shift(1)).sum(axis=1)\n",
    "portfolio_returns = portfolio_returns.groupby(pred_returns[\"year_month\"]).mean()\n",
    "cum_ret = (1 + portfolio_returns).cumprod() - 1\n",
    "\n",
    "portfolio_df = pd.DataFrame({\n",
    "    \"year_month\": portfolio_returns.index.astype(str),\n",
    "    \"portfolio\": portfolio_returns.values\n",
    "})\n",
    "portfolio_df[\"year\"] = portfolio_df[\"year_month\"].str[:4].astype(int)\n",
    "portfolio_df[\"month\"] = portfolio_df[\"year_month\"].str[5:].astype(int)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4. Calculations of performance metrics\n",
    "# -----------------------------------------------\n",
    "\n",
    "def SharpeRatio(returns):\n",
    "    return returns.mean() / returns.std() * np.sqrt(12)\n",
    "\n",
    "def maxDrawDown(cum_returns):\n",
    "    running_max = np.maximum.accumulate(cum_returns + 1)\n",
    "    dd = (cum_returns + 1) / running_max - 1\n",
    "    return dd.min()  # most negative drawdown\n",
    "\n",
    "def annualizedReturn(returns):\n",
    "    return returns.mean() * 12\n",
    "\n",
    "def annualizedStd(returns):\n",
    "    return returns.std() * np.sqrt(12)\n",
    "\n",
    "def maxmonthLoss(returns):\n",
    "    return returns.min()\n",
    "\n",
    "def annualizedAlphaInfoTstat(returns_df, file_path):\n",
    "    mkt = pd.read_csv(file_path)\n",
    "    mkt['year'] = mkt['year'].astype(int)\n",
    "    mkt['month'] = mkt['month'].astype(int)\n",
    "    merged_df = returns_df.merge(mkt, how=\"inner\", on=[\"year\", \"month\"])\n",
    "    merged_df['port_excess'] = merged_df['portfolio'] - merged_df['rf']\n",
    "\n",
    "    nw_ols = sm.ols(formula=\"port_excess ~ mkt_rf\", data=merged_df).fit(\n",
    "        cov_type=\"HAC\", cov_kwds={\"maxlags\": 3}, use_t=True\n",
    "    )\n",
    "\n",
    "    alpha = nw_ols.params[\"Intercept\"] * 12  # annualized alpha\n",
    "    t_statistic = nw_ols.tvalues[\"Intercept\"]\n",
    "    info_ratio = alpha / (np.sqrt(nw_ols.mse_resid) * np.sqrt(12))\n",
    "\n",
    "    return alpha, t_statistic, info_ratio\n",
    "\n",
    "# compute metrics\n",
    "Sharpe_Annualized = SharpeRatio(portfolio_returns)\n",
    "max_drawdown = maxDrawDown(cum_ret)\n",
    "annual_return = annualizedReturn(portfolio_returns)\n",
    "annual_std = annualizedStd(portfolio_returns)\n",
    "max_loss = maxmonthLoss(portfolio_returns)\n",
    "avg_turnover = np.nanmean(turnover_series)\n",
    "\n",
    "mkt_csv_path = os.path.join(work_dir, \"Quant_Data_Labels/mkt_factor.csv\")\n",
    "alpha, t_stat, info_ratio = annualizedAlphaInfoTstat(portfolio_df, mkt_csv_path)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 5. Results\n",
    "# -----------------------------------------------\n",
    "\n",
    "print(f\"Annualized Sharpe ratio: {Sharpe_Annualized:.3f}\")\n",
    "print(f\"Annualized return: {annual_return:.3%}\")\n",
    "print(f\"Annualized volatility: {annual_std:.3%}\")\n",
    "print(f\"Maximum drawdown: {max_drawdown:.3%}\")\n",
    "print(f\"Maximum monthly loss: {max_loss:.3%}\")\n",
    "print(f\"Average monthly turnover: {avg_turnover:.3%}\")\n",
    "print(f\"Annualized alpha: {alpha:.3%}, t-stat: {t_stat:.2f}, Info ratio: {info_ratio:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6507ec8-6fc6-4550-a57a-ef491255b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_int(timestamp):\n",
    "    \"\"\"Convert timestamp to integer format YYYYMMDD\"\"\"\n",
    "    if isinstance(timestamp, pd.Timestamp):\n",
    "        dt = timestamp.to_pydatetime()\n",
    "    elif isinstance(timestamp, datetime):\n",
    "        dt = timestamp\n",
    "    else:\n",
    "        # if it's a string, parse it first\n",
    "        dt = pd.to_datetime(timestamp)\n",
    "    \n",
    "    # format as YYYYMMDD integer\n",
    "    return int(dt.strftime('%Y%m%d'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
